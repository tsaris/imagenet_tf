#!/bin/bash
# Begin LSF directives
#BSUB -P stf011
#BSUB -J imagenet
#BSUB -o logs/imagenet.o%J
#BSUB -W 2:00
#BSUB -nnodes 1
#BSUB -alloc_flags "nvme smt4"
#BSUB -N
#BSUB -q debug
# End LSF directives and begin shell commands


nnodes=$(cat ${LSB_DJOB_HOSTFILE} | sort | uniq | grep -v login | grep -v batch | wc -l)

module load ibm-wml-ce/1.7.1.a0-0

#jsrun -n${nnodes} -a6 -c42 -g6 -r1 --bind=proportional-packed:5 --launch_distribution=packed \
jsrun -n${nnodes} -a1 -c42 -g1 -r1 --bind=proportional-packed:5 --launch_distribution=packed \
    python resnet_ctl_imagenet_main.py \
    --base_learning_rate=9.5 \
    --batch_size=312 \
    --data_dir=/gpfs/alpine/world-shared/stf011/atsaris/imagenet_all/ \
    --datasets_num_private_threads=32 \
    --dtype=fp16 --enable_device_warmup \
    --enable_eager \
    --epochs_between_evals=4 \
    --eval_dataset_cache \
    --eval_offset_epochs=2 \
    --eval_prefetch_batchs=192 \
    --label_smoothing=0.1 \
    --log_steps=125 \
    --lr_schedule=polynomial \
    --model_dir=/gpfs/alpine/world-shared/stf011/atsaris/mlperf_imagenet_output/ \
    --num_gpus=1 \
    --optimizer=LARS \
    --noreport_accuracy_metrics \
    --single_l2_loss_op \
    --steps_per_loop=514 \
    --tf_gpu_thread_mode=gpu_private \
    --train_epochs=10 \
    --training_dataset_cache \
    --training_prefetch_batchs=128 \
    --verbosity=0 \
    --warmup_epochs=4 \
    --weight_decay=0.0002 \
    --distribution_strategy="off"

